{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Installing Required Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install pandas\n",
    "%pip install stanfordnlp\n",
    "%pip install senticnet\n",
    "%pip install sentistrength\n",
    "%pip install nltk\n",
    "%pip install spacy\n",
    "%pip install sklearn\n",
    "%pip install numpy\n",
    "# run this in the terminal\n",
    "# python -m spacy download en_core_web_sm"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing the Relevant Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "from stanfordcorenlp import StanfordCoreNLP\n",
    "import requests\n",
    "from senticnet.senticnet import SenticNet\n",
    "from sentistrength import PySentiStr\n",
    "from sklearn.metrics import accuracy_score\n",
    "import nltk\n",
    "import spacy\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import re\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import numpy as np\n",
    "from sklearn.svm import SVC\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "nltk.download('punkt')\n",
    "spacyNLP = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stanford NLP for Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download the zip file from the link https://drive.google.com/file/d/1yvCpB2URy0iFjQPn3RmidNOryTlo6vHG/view?usp=share_link\n",
    "# extract the zip file and place the folder in the same directory as this file then cd into the folder\n",
    "# run the following command in the terminal to start the server\n",
    "# java -mx4g -cp \"*\" edu.stanford.stanfordNLP.pipeline.StanfordCoreNLPServer -port {8000 or any port} -timeout 30000\n",
    "# can speed it up by replace 4g with 8g (it represents the ram being used in gigs)\n",
    "stanfordNLP = StanfordCoreNLP(\"http://localhost\", port=8000, timeout=30000)\n",
    "def lemmatize(text):\n",
    "    # perform lemmatization\n",
    "    lemmas = []\n",
    "    output = stanfordNLP.annotate(text, properties={'annotators': 'tokenize,lemma', 'outputFormat': 'json'})\n",
    "    output_dict = json.loads(output)\n",
    "    tokens = output_dict['sentences'][0]['tokens']\n",
    "    for token in tokens:\n",
    "        lemmas.append(token['lemma'])\n",
    "   \n",
    "    return lemmas  "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Converting the given JSON file into actual JSON format for easier readbility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "writeFile = open(\"Sarcasm_Headlines.json\", \"w\")\n",
    "writeFile.write(\"{ \\\"headlines\\\": [\")\n",
    "with open(\"Sarcasm_Headlines_Dataset.json\") as readFile:\n",
    "  for item in readFile:\n",
    "    writeFile.write(item + \",\")\n",
    "# removed the final comma manually\n",
    "writeFile.write(\"]}\")\n",
    "readFile.close()\n",
    "writeFile.close()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing Stage"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reading the dataset and removing all article links as our goal is to analyze the headlines for sarcasm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = json.load(open(\"Sarcasm_Headlines.json\"))\n",
    "df = pd.DataFrame(dataset[\"headlines\"])\n",
    "df.drop([\"article_link\"], axis = 1, inplace = True)\n",
    "df.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### lemmatizing the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "idioms = []\n",
    "with open(\"idioms.txt\") as file:\n",
    "    for line in file:\n",
    "        idioms.append(line.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatizeDataset():\n",
    "    for index, row in df.iterrows():\n",
    "        sentence = row['headline']\n",
    "        row['headline'] = lemmatize(sentence)\n",
    "\n",
    "lemmatizeDataset()\n",
    "df.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### writing to a csv file to avoid having to perform pre-processing again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('lemmatized.csv', index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading the CSV file directly (Lemmatized.csv) has the lemmatized dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>headline</th>\n",
       "      <th>is_sarcastic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>former versace store clerk sues over secret 'b...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>the 'roseanne' revival catches up to our thorn...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>mom starting to fear son's web series closest ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>boehner just wants wife to listen, not come up...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>j.k. rowling wishes snape happy birthday in th...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            headline  is_sarcastic\n",
       "0  former versace store clerk sues over secret 'b...             0\n",
       "1  the 'roseanne' revival catches up to our thorn...             0\n",
       "2  mom starting to fear son's web series closest ...             1\n",
       "3  boehner just wants wife to listen, not come up...             1\n",
       "4  j.k. rowling wishes snape happy birthday in th...             0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"lemmatized.csv\")\n",
    "df.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 1 => Concept Level and Common Sense Knowledge\n",
    "### ConceptNet\n",
    "ConceptNet is a semantic network consisting of common-sense knowledge and concepts, represented<br> in the form of nodes (words or\n",
    "short phrases) and labeled edges (relationships) between them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# set the API endpoint and parameters\n",
    "endpoint = 'http://api.conceptnet.io/c/en/'\n",
    "params = {\n",
    "    'filter': 'core',\n",
    "    'limit': 1000\n",
    "}\n",
    "def conceptNet(sentence):\n",
    "    # send a GET request to the API endpoint\n",
    "    response = requests.get(endpoint + sentence, params=params)\n",
    "\n",
    "    # parse the JSON response\n",
    "    data = json.loads(response.text)\n",
    "    edges = data['edges']\n",
    "    edges.sort(key=lambda x: x['weight'], reverse=True)\n",
    "\n",
    "    return edges"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 2 => Sentiment Score\n",
    "### SentiStrength\n",
    "SentiStrength is a sentiment lexicon that uses linguistic information and rules to detect<br>\n",
    "sentiment strength in English text. SentiStrength provides positive and negative sentiment<br>\n",
    "scores for each word. Both scores are integers from 1 to 5, where 1 signifies weak sentiment<br>\n",
    "and 5 signifies strong sentiment.\n",
    "<br>\n",
    "polarity = positiveSentiment - negativeSentiment\n",
    "\n",
    "### SenticNet\n",
    "SenticNet is a resource for opinion mining that aims to create a collection of commonly<br> \n",
    "used common-sense concepts  with positive and negative sentiment scores. The sentiment <br>\n",
    "score for each word is scaled from -1 to 1, where -1 signifies strongly negative sentiment,<br>\n",
    "0 signifies neutral sentiment and 1 signifies strong positive sentiment.\n",
    "<br> sentiment = score * 5 (in-order to keep it with sentiStrength)\n",
    "\n",
    "### Rules of w_score (sentiment score) selection:\n",
    "- if word belongs to SentiStrength || SenticNet => pick the score whichever exists\n",
    "- if word belongs to SentiStrength && SenticNet => avg score of the lexicons\n",
    "- else get the concepts from concept net to expand the meaning => select top 5 ranked and calculate the avg sentiment score\n",
    "\n",
    "### Final Calculation\n",
    "sum_pos_score = sum of all positive sentiment scores<br>\n",
    "sum_neg_score = sum of all negative sentiment scores<br>\n",
    "if sum_pos_score && sum_neg_score > 0, there is a contradiction in the sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "sn = SenticNet()\n",
    "def senticNetScore(word):\n",
    "    try:\n",
    "        polarityValue = sn.polarity_value(word)\n",
    "        return float(polarityValue) * 5\n",
    "    except KeyError:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "senti = PySentiStr()\n",
    "# got the jar file and data folder from the author (also reverse engineered the pysenti package to extract the jar file)\n",
    "senti.setSentiStrengthPath('D:/Sarcasm_Detection-Feature_Selection/SentiStrengthCom.jar')\n",
    "senti.setSentiStrengthLanguageFolderPath('D:/Sarcasm_Detection-Feature_Selection/SentStrength_Data')\n",
    "def sentiStrengthScore(word):\n",
    "    result = senti.getSentiment(word)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wScore(word):\n",
    "    senticNet = senticNetScore(word)\n",
    "    sentiStrength = sentiStrengthScore(word)[0]\n",
    "    if senticNet == None and sentiStrength == None:\n",
    "        expansion = conceptNet(word)\n",
    "        if len(expansion) == 0:\n",
    "            return 0\n",
    "        else:\n",
    "            score = 0\n",
    "            expansion = expansion[:5]\n",
    "            for edge in expansion:\n",
    "                score += wScore(edge['end']['label'])\n",
    "            return score / 5\n",
    "    elif senticNet == None:\n",
    "        return sentiStrength\n",
    "    elif sentiStrength == None:\n",
    "        return senticNet\n",
    "    else:\n",
    "        return (senticNet + sentiStrength) / 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def positiveScore(results):\n",
    "    score = 0\n",
    "    for result in results:\n",
    "        if result > 0:\n",
    "            score += result\n",
    "    return score\n",
    "def negativeScore(results):\n",
    "    score = 0\n",
    "    for result in results:\n",
    "        if result < 0:\n",
    "            score += result\n",
    "    return score"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 3 => Sentence Coherence\n",
    "Checking the coreference between subjects or objects of a sentence\n",
    "<br> for two subjects w1 and w2, sentence is coherent if\n",
    "- if w1 is antecedent of w2\n",
    "- if w1 and w2 are identical pronouns\n",
    "- if w1 and w2 are identical subjects\n",
    "- w2 starts with the word \"the\" (Definite Noun Phrase)\n",
    "- w2 starts with \"this\", \"that\", \"these\", \"those\" (Demonstrative Noun Phrases)\n",
    "- if w1 and w2 are proper nouns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extractSubject(sentence):\n",
    "    doc = spacyNLP(sentence)\n",
    "    subject = None\n",
    "    for token in doc:\n",
    "        if token.dep_ == \"nsubj\":\n",
    "            subject = token.text\n",
    "    return subject"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hasAntecedents(text):\n",
    "    doc = spacyNLP(text)\n",
    "    antecedents = []\n",
    "    for token in doc:\n",
    "        if token.dep_ == \"nsubj\" and token.head.pos_ == \"VERB\":\n",
    "            for mention in doc.ents:\n",
    "                if mention.start <= token.i < mention.end:\n",
    "                    antecedents.append(mention.text)\n",
    "    return True if len(antecedents) > 0 else False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "pronounLemmatizer = WordNetLemmatizer()\n",
    "def identicalPronouns(w1, w2):\n",
    "    lemma1 = pronounLemmatizer.lemmatize(w1, 'n')\n",
    "    lemma2 = pronounLemmatizer.lemmatize(w2, 'n')\n",
    "    if lemma1 == lemma2:\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def identicalSubjects(w1,w2):\n",
    "    cleanedSubject1 = re.sub(r'[^a-zA-Z]', '', w1)\n",
    "    cleanedSubject2 = re.sub(r'[^a-zA-Z]', '', w2)\n",
    "    if cleanedSubject1 == cleanedSubject2:\n",
    "        return True\n",
    "    else:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def definiteNounPhraseFeature(text,w2):\n",
    "    doc = nltk.word_tokenize(text)\n",
    "    for i in range(len(doc)):\n",
    "        if i-1 >= 0 and doc[i] == w2:\n",
    "            if doc[i-1] == 'the':\n",
    "                return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def demonstrativeNounPhraseFeature(text,w2):\n",
    "    doc = nltk.word_tokenize(text)\n",
    "    for i in range(len(doc)):\n",
    "        if doc[i] == w2:\n",
    "            if i-1 >= 0 and doc[i-1] == 'this' or doc[i-1] == 'that' or doc[i-1] == 'these' or doc[i-1] == 'those':\n",
    "                return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def properNameFeature(w1,w2):\n",
    "    taggedWords = nltk.pos_tag([w1,w2])\n",
    "    proper = False\n",
    "    for word, tag in taggedWords:\n",
    "        if tag in ['NNP', 'NNPS']:\n",
    "            proper = True\n",
    "        else:\n",
    "            proper = False\n",
    "            break\n",
    "    return proper"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 4 => Creation of Feature Vector"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Binary Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"CONTRA\"] = np.zeros(len(df))\n",
    "df[\"CONTRA_PLUS_COHER\"] = np.zeros(len(df))\n",
    "df[\"pos_low\"] = np.zeros(len(df))\n",
    "df[\"pos_med\"] = np.zeros(len(df))\n",
    "df[\"pos_high\"] = np.zeros(len(df))\n",
    "df[\"neg_low\"] = np.zeros(len(df))\n",
    "df[\"neg_med\"] = np.zeros(len(df))\n",
    "df[\"neg_high\"] = np.zeros(len(df))\n",
    "df[\"emo_low\"] = np.zeros(len(df))\n",
    "df[\"emo_med\"] = np.zeros(len(df))\n",
    "df[\"emo_high\"] = np.zeros(len(df))\n",
    "df[\"rep_punc_low\"] = np.zeros(len(df))\n",
    "df[\"rep_punc_med\"] = np.zeros(len(df))\n",
    "df[\"rep_punc_high\"] = np.zeros(len(df))\n",
    "df[\"rep_seq_low\"] = np.zeros(len(df))\n",
    "df[\"rep_seq_med\"] = np.zeros(len(df))\n",
    "df[\"rep_seq_high\"] = np.zeros(len(df))\n",
    "df[\"cap_low\"] = np.zeros(len(df))\n",
    "df[\"cap_med\"] = np.zeros(len(df))\n",
    "df[\"cap_high\"] = np.zeros(len(df))\n",
    "df[\"slang_low\"] = np.zeros(len(df))\n",
    "df[\"slang_med\"] = np.zeros(len(df))\n",
    "df[\"slang_high\"] = np.zeros(len(df))\n",
    "df[\"exclaim_low\"] = np.zeros(len(df))\n",
    "df[\"exclaim_med\"] = np.zeros(len(df))\n",
    "df[\"exclaim_high\"] = np.zeros(len(df))\n",
    "df[\"idioms_low\"] = np.zeros(len(df))\n",
    "df[\"idioms_med\"] = np.zeros(len(df))\n",
    "df[\"idioms_high\"] = np.zeros(len(df))\n",
    "boosterAndSlangs = [\"Lit\", \"Fleek\", \"Slay\", \"Woke\", \"Stan\", \"Chill\", \"On fleek\", \"Squad\", \"Bae\", \"AF\", \"Savage\", \"GOAT\", \"Lit AF\", \"Yas\", \"Gucci\", \"Thirsty\", \"Mood\", \"Extra\", \"Clap back\", \"Shook\", \"Lowkey\", \"Highkey\", \"Basic\", \"Lituation\", \"Snatched\", \"Throwing shade\", \"Swag\", \"Tea\", \"Glow up\", \"Fam\", \"Turnt\", \"Litty\", \"Dope\", \"Hundo P\", \"Gassed\", \"FOMO\", \"Trill\", \"No cap\", \"Blessed\", \"Fire\", \"Wavy\", \"Sus\", \"Tight\", \"Meme\", \"Shade\", \"Receipts\", \"Slay queen\", \"Cray\", \"Thick\", \"Litmas\", \"Litmus\", \"Queen\", \"Bad\", \"No chill\", \"Sorry not sorry\", \"Real talk\", \"Dank\", \"Ship\", \"Ratchet\", \"Yolo\", \"Fierce\", \"Legendary\", \"Drama\", \"Stuntin\", \"Lit fam\", \"Flame\", \"Finna\", \"Swole\", \"Squad goals\", \"Kween\", \"Salty\", \"Slaying\", \"Bounce\", \"Swerve\", \"Bussin\", \"Hype\", \"Finesse\", \"Bless up\", \"Crushin it\", \"Yaas\", \"Fleeky\", \"Fuego\", \"Cringy\", \"Dead\", \"Curve\", \"Baller\", \"Wig snatched\", \"Keep it 100\", \"Hater\", \"My bad\"]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Contradiction Feature: <br>\n",
    "<emsp>We use two binary features Contra and Contra_Coher<br>\n",
    "<emsp>Contra if headline has one sentence and contradiction in sentiment score occur\n",
    "<br>\n",
    "<emsp>Contra_Coher if headline has more than one sentence, contradiction of polarity and the headline is judged coherent<br>\n",
    "\n",
    "### Sentiment Feature <br>\n",
    "<emsp>Calculates the +ve and -ve score of the headline and then classify it as low/med/high\n",
    "\n",
    "### Punctuations and Symbol Features <br>\n",
    "<emsp>We use 7 indicators<br><br>\n",
    "    <emsp><emsp>1. Number of emoticons <br>\n",
    "    <emsp><emsp>2. Number of repetitive sequence of punctuations<br>\n",
    "    <emsp><emsp>3. Number of repetitive sequence of characters<br>\n",
    "    <emsp><emsp>4. Number of capitalized word<br>\n",
    "    <emsp><emsp>5. Number of slang and booster words<br>\n",
    "    <emsp><emsp>6. Number of exclamation marks<br>\n",
    "    <emsp><emsp>7. Number of idioms<br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_symbols(line):\n",
    "    return ''.join(ch for ch in line if ch.isalnum() or ch == \" \")\n",
    "def calculate_scores(sentence):\n",
    "    print(\"Sentence: \",sentence)\n",
    "    score=[]\n",
    "    results = []\n",
    "    for word in nltk.word_tokenize(sentence):\n",
    "        results.append(wScore(word))\n",
    "    positiveSum = positiveScore(results)\n",
    "    negativeSum = negativeScore(results)\n",
    "    score.append(positiveSum)\n",
    "    score.append(negativeSum)\n",
    "    print(\"positiveScore: \",positiveSum)\n",
    "    print(\"negativeScore: \",negativeSum)\n",
    "    return score\n",
    "\n",
    "def isContradiction(scores):\n",
    "    if scores[0]!=0 and scores[1]!=0:\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "def checkCoherence(sentence):\n",
    "    tokens = nltk.sent_tokenize(sentence)\n",
    "    if len(tokens) > 1:\n",
    "        if hasAntecedents(sentence):\n",
    "            return True\n",
    "        w1 = extractSubject(tokens[0])\n",
    "        w2 = extractSubject(tokens[1])\n",
    "        if identicalPronouns(w1,w2) or identicalSubjects(w1,w2) or definiteNounPhraseFeature(tokens[1],w2) or demonstrativeNounPhraseFeature(tokens[1],w2) or properNameFeature(w1,w2):\n",
    "            return True   \n",
    "    return False\n",
    "\n",
    "def countEmoticons(headline):\n",
    "    return len(re.findall(r'[^\\w\\s,]', headline))\n",
    "\n",
    "def countRepititivePunctuations(headline):\n",
    "    return len(re.findall(r'([\\W_]){2,}', headline))\n",
    "\n",
    "def countRepititiveSequences(headline):\n",
    "    return len(re.findall(r'(\\S)\\1{1,}', headline))\n",
    "\n",
    "def countCapitalLetters(headline):\n",
    "    return len(re.findall(r'[A-Z]', headline))\n",
    "\n",
    "def countBoostersAndSlangs(headline):\n",
    "    numSlangsBoosters = 0\n",
    "    for word in headline.split():\n",
    "        if word.lower() in boosterAndSlangs:\n",
    "            numSlangsBoosters += 1\n",
    "    return numSlangsBoosters\n",
    "\n",
    "def countIdioms(headline):\n",
    "    numIdioms = 0\n",
    "    for word in headline.split():\n",
    "        if word.lower() in idioms:\n",
    "            numIdioms += 1\n",
    "    return numIdioms\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def assignSentimentFeature(headline,scores):\n",
    "    positiveScore = scores[0]\n",
    "    negativeScore = scores[1]\n",
    "    if positiveScore <= -1:\n",
    "        df.loc[df[\"headline\"] == headline, \"pos_low\"] = 1\n",
    "    elif positiveScore >= 0 and positiveScore <= 1:\n",
    "        df.loc[df[\"headline\"] == headline, \"pos_med\"] = 1\n",
    "    elif positiveScore >= 2:\n",
    "        df.loc[df[\"headline\"] == headline, \"pos_high\"] = 1\n",
    "    if negativeScore >= 1:\n",
    "        df.loc[df[\"headline\"] == headline, \"neg_low\"] = 1\n",
    "    elif negativeScore >= 0 and negativeScore <= 1:\n",
    "        df.loc[df[\"headline\"] == headline, \"neg_med\"] = 1\n",
    "    elif negativeScore <= -2:\n",
    "        df.loc[df[\"headline\"] == headline, \"neg_high\"] = 1\n",
    "def punctuationAndSpecialSymbolFeature(headline):\n",
    "    numberOfEmoticons = countEmoticons(headline)\n",
    "    if numberOfEmoticons == 0:\n",
    "        df.loc[df[\"headline\"] == headline, \"emo_low\"] = 1\n",
    "    elif numberOfEmoticons >= 1 and numberOfEmoticons <= 3:\n",
    "        df.loc[df[\"headline\"] == headline, \"emo_med\"] = 1\n",
    "    elif numberOfEmoticons >= 4:\n",
    "        df.loc[df[\"headline\"] == headline, \"emo_high\"] = 1\n",
    "    numberOfPunctuations = countRepititivePunctuations(headline)\n",
    "    if numberOfPunctuations == 0:\n",
    "        df.loc[df[\"headline\"] == headline, \"rep_punc_low\"] = 1\n",
    "    elif numberOfPunctuations >= 1 and numberOfPunctuations <= 3:\n",
    "        df.loc[df[\"headline\"] == headline, \"rep_punc_med\"] = 1\n",
    "    elif numberOfPunctuations >= 4:\n",
    "        df.loc[df[\"headline\"] == headline, \"rep_punc_high\"] = 1\n",
    "    numberOfRepetitiveSequences = countRepititiveSequences(headline)\n",
    "    if numberOfRepetitiveSequences == 0:\n",
    "        df.loc[df[\"headline\"] == headline, \"rep_seq_low\"] = 1\n",
    "    elif numberOfRepetitiveSequences >= 1 and numberOfRepetitiveSequences <= 3:\n",
    "        df.loc[df[\"headline\"] == headline, \"rep_seq_med\"] = 1\n",
    "    elif numberOfRepetitiveSequences >= 4:\n",
    "        df.loc[df[\"headline\"] == headline, \"rep_seq_high\"] = 1\n",
    "    numberOfCapitalLetters = countCapitalLetters(headline)\n",
    "    if numberOfCapitalLetters == 0:\n",
    "        df.loc[df[\"headline\"] == headline, \"cap_low\"] = 1\n",
    "    elif numberOfCapitalLetters >= 1 and numberOfCapitalLetters <= 3:\n",
    "        df.loc[df[\"headline\"] == headline, \"cap_med\"] = 1\n",
    "    elif numberOfCapitalLetters >= 4:\n",
    "        df.loc[df[\"headline\"] == headline, \"cap_high\"] = 1\n",
    "    numberOfBoostersAndSlangs = countBoostersAndSlangs(headline)\n",
    "    if numberOfBoostersAndSlangs == 0:\n",
    "        df.loc[df[\"headline\"] == headline, \"slang_low\"] = 1\n",
    "    elif numberOfBoostersAndSlangs >= 1 and numberOfBoostersAndSlangs <= 3:\n",
    "        df.loc[df[\"headline\"] == headline, \"slang_med\"] = 1\n",
    "    elif numberOfBoostersAndSlangs >= 4:\n",
    "        df.loc[df[\"headline\"] == headline, \"slang_high\"] = 1\n",
    "    numberOfIdioms = countIdioms(headline)\n",
    "    if numberOfIdioms == 0:\n",
    "        df.loc[df[\"headline\"] == headline, \"idiom_low\"] = 1\n",
    "    elif numberOfIdioms >= 1 and numberOfIdioms <= 3:\n",
    "        df.loc[df[\"headline\"] == headline, \"idiom_med\"] = 1\n",
    "    elif numberOfIdioms >= 4:\n",
    "        df.loc[df[\"headline\"] == headline, \"idiom_high\"] = 1\n",
    "    \n",
    "def contradictionFeature():\n",
    "    for headline in df[\"headline\"]:\n",
    "        text = remove_symbols(headline)\n",
    "        sentences = nltk.sent_tokenize(text)\n",
    "        scores = calculate_scores(text)\n",
    "        assignSentimentFeature(headline,scores)\n",
    "        punctuationAndSpecialSymbolFeature(headline)\n",
    "        if len(sentences) > 1:\n",
    "            print(\"CONTRA_PLUS_COHER\")\n",
    "            if isContradiction(scores) and checkCoherence(text):\n",
    "                df.loc[df[\"headline\"] == headline, \"CONTRA_PLUS_COHER\"] = 1\n",
    "            else:\n",
    "                df.loc[df[\"headline\"] == headline, \"CONTRA_PLUS_COHER\"] = 0\n",
    "        else:\n",
    "            print(\"CONTRA\")\n",
    "            if isContradiction(scores):\n",
    "                df.loc[df[\"headline\"] == headline, \"CONTRA\"] = 1\n",
    "            else:\n",
    "                df.loc[df[\"headline\"] == headline, \"CONTRA\"] = 0"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Processing Takes a lot of time (took around 40.5 hours) so we had to divide the dataset into pieces of 1000s and perform the feature generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "listDFs = []\n",
    "for i in range(0,26000,1000):\n",
    "    temp = df[i:i+1000]\n",
    "    listDFs.append(temp)\n",
    "listDFs.append(df[26000:])\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving them into temporary files since it was not possible to do it in a single day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rangee= 0\n",
    "for i in range(0,26,1):\n",
    "    df = listDFs[rangee + i]\n",
    "    contradictionFeature()\n",
    "    print(df.head())\n",
    "    df.to_csv(\"tempContra/df\"+str(rangee+1+i)+\".csv\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Concatenating all the temporary files into a single csv (FeatureSet.csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(26709, 32)\n",
      "                                            headline  is_sarcastic  CONTRA   \n",
      "0  former versace store clerk sues over secret 'b...             0     1.0  \\\n",
      "1  the 'roseanne' revival catches up to our thorn...             0     1.0   \n",
      "2  mom starting to fear son's web series closest ...             1     1.0   \n",
      "3  boehner just wants wife to listen, not come up...             1     0.0   \n",
      "4  j.k. rowling wishes snape happy birthday in th...             0     0.0   \n",
      "\n",
      "   CONTRA_PLUS_COHER  pos_low  pos_med  pos_high  neg_low  neg_med  neg_high   \n",
      "0                0.0      0.0      0.0       1.0      0.0      0.0       0.0  \\\n",
      "1                0.0      0.0      0.0       1.0      0.0      0.0       1.0   \n",
      "2                0.0      0.0      0.0       1.0      0.0      0.0       1.0   \n",
      "3                0.0      0.0      0.0       1.0      0.0      1.0       0.0   \n",
      "4                0.0      0.0      0.0       1.0      0.0      1.0       0.0   \n",
      "\n",
      "   ...  slang_low  slang_med  slang_high  exclaim_low  exclaim_med   \n",
      "0  ...        1.0        0.0         0.0          0.0          0.0  \\\n",
      "1  ...        1.0        0.0         0.0          0.0          0.0   \n",
      "2  ...        1.0        0.0         0.0          0.0          0.0   \n",
      "3  ...        1.0        0.0         0.0          0.0          0.0   \n",
      "4  ...        1.0        0.0         0.0          0.0          0.0   \n",
      "\n",
      "   exclaim_high  idioms_low  idioms_med  idioms_high  idiom_low  \n",
      "0           0.0         0.0         0.0          0.0        1.0  \n",
      "1           0.0         0.0         0.0          0.0        1.0  \n",
      "2           0.0         0.0         0.0          0.0        1.0  \n",
      "3           0.0         0.0         0.0          0.0        1.0  \n",
      "4           0.0         0.0         0.0          0.0        1.0  \n",
      "\n",
      "[5 rows x 32 columns]\n"
     ]
    }
   ],
   "source": [
    "length = 27\n",
    "df = pd.read_csv(\"tempContra/df1.csv\")\n",
    "for i in range(1,length):\n",
    "    df1 = pd.read_csv(\"tempContra/df\"+str(i+1)+\".csv\")\n",
    "    df = pd.concat([df,df1])\n",
    "df.drop([\"Unnamed: 0\"],axis=1,inplace=True)\n",
    "print(df.shape)\n",
    "print(df.head())\n",
    "df.to_csv(\"FeatureSet.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"lemmatized.csv\")\n",
    "featureSet = pd.read_csv(\"FeatureSet.csv\")\n",
    "featureSet.drop([\"headline\"],axis=1,inplace=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# N-Grams SVC Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "ngramX = df[\"headline\"]  # Features\n",
    "y = df['is_sarcastic']\n",
    "ngramRange = (1, 3)\n",
    "vectorizer = CountVectorizer(ngram_range=ngramRange)\n",
    "X_vectorized = vectorizer.fit_transform(ngramX)\n",
    "ngramSVM = SVC(kernel='linear')\n",
    "ngramSVM.fit(X_vectorized, y)\n",
    "ngramPredictions = cross_val_predict(ngramSVM, X_vectorized, y, cv=10,n_jobs=4)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Space SVC Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "featureSetX = featureSet.drop('is_sarcastic', axis=1)  # Features\n",
    "featureVectorizer = CountVectorizer()\n",
    "featureX = featureVectorizer.fit_transform(featureSetX)\n",
    "featureSetSVM = SVC(kernel='linear')\n",
    "featureSetSVM.fit(featureSetX, y)\n",
    "featureSetPredictions = cross_val_predict(featureSetSVM, featureSetX, y, cv=10,n_jobs=4)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_metrics(true_labels, predicted_labels):\n",
    "    true_positives = sum((true == 1 and pred == 1) for true, pred in zip(true_labels, predicted_labels))\n",
    "    false_positives = sum((true == 0 and pred == 1) for true, pred in zip(true_labels, predicted_labels))\n",
    "    false_negatives = sum((true == 1 and pred == 0) for true, pred in zip(true_labels, predicted_labels))\n",
    "\n",
    "    recall = true_positives / (true_positives + false_negatives)\n",
    "    precision = true_positives / (true_positives + false_positives)\n",
    "    f1_score = 2 * (precision * recall) / (precision + recall)\n",
    "\n",
    "    return recall, precision, f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Definition Accuracy:  0.552510389756262\n",
      "Definition Recall:  0.5614977823268509\n",
      "Definition Precision:  0.4914887262953561\n",
      "Definition F1 Score:  0.524165936778406\n"
     ]
    }
   ],
   "source": [
    "definitionResults=[]\n",
    "for index, row in featureSet.iterrows():\n",
    "    if row['CONTRA'] == 1 or row['CONTRA_PLUS_COHER'] == 1:\n",
    "        definitionResults.append(1)\n",
    "    else:\n",
    "        definitionResults.append(0)\n",
    "recall, precision, f1_score = calculate_metrics(y, definitionResults)\n",
    "definitionAccuracy = accuracy_score(y, definitionResults)\n",
    "print(\"Definition Accuracy: \",definitionAccuracy)\n",
    "print(\"Definition Recall: \",recall)\n",
    "print(\"Definition Precision: \",precision)\n",
    "print(\"Definition F1 Score: \",f1_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N-gram Recall:  0.8239508700102354\n",
      "N-gram Precision:  0.8469969311705392\n",
      "N-gram F1 Score:  0.8353149725452895\n",
      "N-gram Accuracy Score:  0.8573888951289828\n"
     ]
    }
   ],
   "source": [
    "# Only N-grams\n",
    "ngramAccuracyScore = accuracy_score(y, ngramPredictions)\n",
    "ngramRecall, ngramPrecision, ngramF1Score = calculate_metrics(y, ngramPredictions)\n",
    "print(\"N-gram Recall: \", ngramRecall)\n",
    "print(\"N-gram Precision: \", ngramPrecision)\n",
    "print(\"N-gram F1 Score: \", ngramF1Score)\n",
    "print(\"N-gram Accuracy Score: \", ngramAccuracyScore)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature Set Recall:  0.5931422722620266\n",
      "Feature Set Precision:  0.5385270657476962\n",
      "Feature Set F1 Score:  0.564516783699314\n",
      "Feature Set Accuracy Score:  0.5983001984349845\n"
     ]
    }
   ],
   "source": [
    "# Only Feature Set\n",
    "featureSetAccuracyScore = accuracy_score(y, featureSetPredictions)\n",
    "featureSetRecall, featureSetPrecision, featureSetF1Score = calculate_metrics(y, featureSetPredictions)\n",
    "print(\"Feature Set Recall: \", featureSetRecall)\n",
    "print(\"Feature Set Precision: \", featureSetPrecision)\n",
    "print(\"Feature Set F1 Score: \", featureSetF1Score)\n",
    "print(\"Feature Set Accuracy Score: \", featureSetAccuracyScore)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N-grams + Feature Set Recall:  0.7651825315591948\n",
      "N-grams + Feature Set Precision:  0.8506542765029395\n",
      "N-grams + Feature Set F1 Score:  0.8056578356533451\n",
      "N-grams + Feature Set Accuracy Score:  0.8379572428769329\n"
     ]
    }
   ],
   "source": [
    "# N-grams + Feature Set\n",
    "finalResults = []\n",
    "for i in range(len(ngramPredictions)):\n",
    "    if ngramPredictions[i] == featureSetPredictions[i]:\n",
    "        finalResults.append(ngramPredictions[i])\n",
    "    else:\n",
    "        margin_ngrams = ngramSVM.decision_function(X_vectorized[i]) \n",
    "        margin_features = featureSetSVM.decision_function([featureSetX.iloc[i]])  # Remove the wrapping of [featureX[i]] in square brackets\n",
    "        if abs(margin_ngrams) > abs(margin_features):\n",
    "            finalResults.append(ngramPredictions[i])\n",
    "        else:\n",
    "            finalResults.append(featureSetPredictions[i]) \n",
    "accuracy = accuracy_score(y, finalResults)\n",
    "recall, precision, f1_score = calculate_metrics(y, finalResults)\n",
    "print(\"N-grams + Feature Set Recall: \", recall)\n",
    "print(\"N-grams + Feature Set Precision: \", precision)\n",
    "print(\"N-grams + Feature Set F1 Score: \", f1_score)\n",
    "print(\"N-grams + Feature Set Accuracy Score: \", accuracy)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table>\n",
    "  <tr>\n",
    "    <th>Number</th>\n",
    "    <th>Method</th>\n",
    "    <th>Precision</th>\n",
    "    <th>Recall</th>\n",
    "    <th>F-Score</th>\n",
    "    <th>Accuracy</th>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>1.</td>\n",
    "    <td>Contradiction in Sentiment Score</td>\n",
    "    <td>49.14%</td>\n",
    "    <td>56.14%</td>\n",
    "    <td>52.41%</td>\n",
    "    <td>55.25%</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>2.</td>\n",
    "    <td>Uni-gram, bi-gram and tri-gram features</td>\n",
    "    <td>84.69%</td>\n",
    "    <td>82.39%</td>\n",
    "    <td>83.53%</td>\n",
    "    <td>85.73%</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>3.</td>\n",
    "    <td>Proposed Features</td>\n",
    "    <td>53.85%</td>\n",
    "    <td>59.31%</td>\n",
    "    <td>56.45%</td>\n",
    "    <td>59.83%</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>4.</td>\n",
    "    <td>N-Grams and Proposed Features Combined</td>\n",
    "    <td>85.06%</td>\n",
    "    <td>76.51%</td>\n",
    "    <td>80.56%</td>\n",
    "    <td>83.79%</td>\n",
    "  </tr>\n",
    "</table>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "\n",
    " From the above analysis, it can be observed that N-grams were the best baselines to predict sarcasm in<br> headlines as opposed to our generated features due to some reasons like no use of emojis in headlines, and their lengths being short<br> as well with little to no punctuations, although the combination of N-Grams with our proposed features gave us an accuracy closer to<br> the n-grams version, it can be said that the combination is a great measure for classifying headlines as sarcastic."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
