{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install pandas\n",
    "%pip install stanfordnlp\n",
    "%pip install senticnet\n",
    "%pip install sentistrength\n",
    "%pip install nltk\n",
    "%pip install spacy\n",
    "%pip install sklearn\n",
    "%pip install numpy\n",
    "# run this in the terminal\n",
    "# python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "from stanfordcorenlp import StanfordCoreNLP\n",
    "import requests\n",
    "from senticnet.senticnet import SenticNet\n",
    "from sentistrength import PySentiStr\n",
    "import nltk\n",
    "import spacy\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import re\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('punkt')\n",
    "# stanfordNLP = StanfordCoreNLP(\"http://localhost\", port=8000, timeout=30000)\n",
    "spacyNLP = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download the zip file from the link https://drive.google.com/file/d/1yvCpB2URy0iFjQPn3RmidNOryTlo6vHG/view?usp=share_link\n",
    "# extract the zip file and place the folder in the same directory as this file then cd into the folder\n",
    "# run the following command in the terminal to start the server\n",
    "# java -mx4g -cp \"*\" edu.stanford.stanfordNLP.pipeline.StanfordCoreNLPServer -port {8000 or any port} -timeout 30000\n",
    "# can speed it up by replace 4g with 8g (it represents the ram being used in gigs)\n",
    "def lemmatize(text):\n",
    "    # perform lemmatization\n",
    "    lemmas = []\n",
    "    output = stanfordNLP.annotate(text, properties={'annotators': 'tokenize,lemma', 'outputFormat': 'json'})\n",
    "    output_dict = json.loads(output)\n",
    "    tokens = output_dict['sentences'][0]['tokens']\n",
    "    for token in tokens:\n",
    "        lemmas.append(token['lemma'])\n",
    "   \n",
    "    return lemmas  "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Converting the given JSON file into actual JSON format for easier readbility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "writeFile = open(\"Sarcasm_Headlines.json\", \"w\")\n",
    "writeFile.write(\"{ \\\"headlines\\\": [\")\n",
    "with open(\"Sarcasm_Headlines_Dataset.json\") as readFile:\n",
    "  for item in readFile:\n",
    "    writeFile.write(item + \",\")\n",
    "# removed the final comma manually\n",
    "writeFile.write(\"]}\")\n",
    "readFile.close()\n",
    "writeFile.close()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing Stage"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reading the dataset and removing all article links as our goal is to analyze the headlines for sarcasm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = json.load(open(\"Sarcasm_Headlines.json\"))\n",
    "df = pd.DataFrame(dataset[\"headlines\"])\n",
    "df.drop([\"article_link\"], axis = 1, inplace = True)\n",
    "df.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### lemmatizing the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatizeDataset():\n",
    "    for index, row in df.iterrows():\n",
    "        sentence = row['headline']\n",
    "        row['headline'] = lemmatize(sentence)\n",
    "\n",
    "lemmatizeDataset()\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idioms = []\n",
    "with open(\"idioms.txt\") as file:\n",
    "    for line in file:\n",
    "        idioms.append(line.strip())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### writing to a csv file to avoid having to perform pre-processing again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('lemmatized.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"lemmatized.csv\")\n",
    "df.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 1 => Concept Level and Common Sense Knowledge\n",
    "### ConceptNet\n",
    "ConceptNet is a semantic network consisting of common-sense knowledge and concepts, represented<br> in the form of nodes (words or\n",
    "short phrases) and labeled edges (relationships) between them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# set the API endpoint and parameters\n",
    "endpoint = 'http://api.conceptnet.io/c/en/'\n",
    "params = {\n",
    "    'filter': 'core',\n",
    "    'limit': 1000\n",
    "}\n",
    "def conceptNet(sentence):\n",
    "    # send a GET request to the API endpoint\n",
    "    response = requests.get(endpoint + sentence, params=params)\n",
    "\n",
    "    # parse the JSON response\n",
    "    data = json.loads(response.text)\n",
    "    edges = data['edges']\n",
    "    edges.sort(key=lambda x: x['weight'], reverse=True)\n",
    "\n",
    "    return edges"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 2 => Sentiment Score\n",
    "### SentiStrength\n",
    "SentiStrength is a sentiment lexicon that uses linguistic information and rules to detect<br>\n",
    "sentiment strength in English text. SentiStrength provides positive and negative sentiment<br>\n",
    "scores for each word. Both scores are integers from 1 to 5, where 1 signifies weak sentiment<br>\n",
    "and 5 signifies strong sentiment.\n",
    "<br>\n",
    "polarity = positiveSentiment - negativeSentiment\n",
    "\n",
    "### SenticNet\n",
    "SenticNet is a resource for opinion mining that aims to create a collection of commonly<br> \n",
    "used common-sense concepts  with positive and negative sentiment scores. The sentiment <br>\n",
    "score for each word is scaled from -1 to 1, where -1 signifies strongly negative sentiment,<br>\n",
    "0 signifies neutral sentiment and 1 signifies strong positive sentiment.\n",
    "<br> sentiment = score * 5 (in-order to keep it with sentiStrength)\n",
    "\n",
    "### Rules of w_score (sentiment score) selection:\n",
    "- if word belongs to SentiStrength || SenticNet => pick the score whichever exists\n",
    "- if word belongs to SentiStrength && SenticNet => avg score of the lexicons\n",
    "- else get the concepts from concept net to expand the meaning => select top 5 ranked and calculate the avg sentiment score\n",
    "\n",
    "### Final Calculation\n",
    "sum_pos_score = sum of all positive sentiment scores<br>\n",
    "sum_neg_score = sum of all negative sentiment scores<br>\n",
    "if sum_pos_score && sum_neg_score > 0, there is a contradiction in the sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sn = SenticNet()\n",
    "def senticNetScore(word):\n",
    "    try:\n",
    "        polarityValue = sn.polarity_value(word)\n",
    "        return float(polarityValue) * 5\n",
    "    except KeyError:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "senti = PySentiStr()\n",
    "# got the jar file and data folder from the author (also reverse engineered the pysenti package to extract the jar file)\n",
    "senti.setSentiStrengthPath('C:/Users/pd/OneDrive/Desktop/IR project/Sarcasm_Detection-Feature_Selection/SentiStrengthCom.jar')\n",
    "senti.setSentiStrengthLanguageFolderPath('C:/Users/pd/OneDrive/Desktop/IR project/Sarcasm_Detection-Feature_Selection/SentStrength_Data')\n",
    "def sentiStrengthScore(word):\n",
    "    result = senti.getSentiment(word)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wScore(word):\n",
    "    senticNet = senticNetScore(word)\n",
    "    sentiStrength = sentiStrengthScore(word)[0]\n",
    "    if senticNet == None and sentiStrength == None:\n",
    "        expansion = conceptNet(word)\n",
    "        if len(expansion) == 0:\n",
    "            return 0\n",
    "        else:\n",
    "            score = 0\n",
    "            expansion = expansion[:5]\n",
    "            for edge in expansion:\n",
    "                score += wScore(edge['end']['label'])\n",
    "            return score / 5\n",
    "    elif senticNet == None:\n",
    "        return sentiStrength\n",
    "    elif sentiStrength == None:\n",
    "        return senticNet\n",
    "    else:\n",
    "        return (senticNet + sentiStrength) / 2\n",
    "\n",
    "\n",
    "\n",
    "# print(wScore('cricket')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def positiveScore(results):\n",
    "    score = 0\n",
    "    for result in results:\n",
    "        if result > 0:\n",
    "            score += result\n",
    "    return score\n",
    "def negativeScore(results):\n",
    "    score = 0\n",
    "    for result in results:\n",
    "        if result < 0:\n",
    "            score += result\n",
    "    return score"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 3 => Sentence Coherence\n",
    "Checking the coreference between subjects or objects of a sentence\n",
    "<br> for two subjects w1 and w2, sentence is coherent if\n",
    "- if w1 is antecedent of w2\n",
    "- if w1 and w2 are identical pronouns\n",
    "- if w1 and w2 are identical subjects\n",
    "- w2 starts with the word \"the\" (Definite Noun Phrase)\n",
    "- w2 starts with \"this\", \"that\", \"these\", \"those\" (Demonstrative Noun Phrases)\n",
    "- if w1 and w2 are proper nouns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extractSubject(sentence):\n",
    "    doc = spacyNLP(sentence)\n",
    "    subject = None\n",
    "    for token in doc:\n",
    "        if token.dep_ == \"nsubj\":\n",
    "            subject = token.text\n",
    "    return subject"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hasAntecedents(text):\n",
    "    doc = spacyNLP(text)\n",
    "    antecedents = []\n",
    "    for token in doc:\n",
    "        if token.dep_ == \"nsubj\" and token.head.pos_ == \"VERB\":\n",
    "            for mention in doc.ents:\n",
    "                if mention.start <= token.i < mention.end:\n",
    "                    antecedents.append(mention.text)\n",
    "    return True if len(antecedents) > 0 else False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pronounLemmatizer = WordNetLemmatizer()\n",
    "def identicalPronouns(w1, w2):\n",
    "    lemma1 = pronounLemmatizer.lemmatize(w1, 'n')\n",
    "    lemma2 = pronounLemmatizer.lemmatize(w2, 'n')\n",
    "    if lemma1 == lemma2:\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def identicalSubjects(w1,w2):\n",
    "    cleanedSubject1 = re.sub(r'[^a-zA-Z]', '', w1)\n",
    "    cleanedSubject2 = re.sub(r'[^a-zA-Z]', '', w2)\n",
    "    if cleanedSubject1 == cleanedSubject2:\n",
    "        return True\n",
    "    else:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def definiteNounPhraseFeature(text,w2):\n",
    "    doc = nltk.word_tokenize(text)\n",
    "    for i in range(len(doc)):\n",
    "        if i-1 >= 0 and doc[i] == w2:\n",
    "            if doc[i-1] == 'the':\n",
    "                return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def demonstrativeNounPhraseFeature(text,w2):\n",
    "    doc = nltk.word_tokenize(text)\n",
    "    for i in range(len(doc)):\n",
    "        if doc[i] == w2:\n",
    "            if i-1 >= 0 and doc[i-1] == 'this' or doc[i-1] == 'that' or doc[i-1] == 'these' or doc[i-1] == 'those':\n",
    "                return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def properNameFeature(w1,w2):\n",
    "    taggedWords = nltk.pos_tag([w1,w2])\n",
    "    proper = False\n",
    "    for word, tag in taggedWords:\n",
    "        if tag in ['NNP', 'NNPS']:\n",
    "            proper = True\n",
    "        else:\n",
    "            proper = False\n",
    "            break\n",
    "    return proper"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 4 => Creation of Feature Vector"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating N-gram Feature Spaces\n",
    "- Baseline 1 => unigram space\n",
    "- Baseline 2 => unigram, bigram and trigram space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createFeatureSpaces():\n",
    "    sentences = df[\"headline\"]\n",
    "    vectorizer1 = CountVectorizer(ngram_range=(1,1))\n",
    "    vectorizer2 = CountVectorizer(ngram_range=(1,3))\n",
    "    featureSpace1 = vectorizer1.fit_transform(sentences)\n",
    "    featureSpace2 = vectorizer2.fit_transform(sentences)\n",
    "    return (vectorizer1, featureSpace1), (vectorizer2, featureSpace2)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Binary Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"CONTRA\"] = np.zeros(len(df))\n",
    "df[\"CONTRA_PLUS_COHER\"] = np.zeros(len(df))\n",
    "df[\"pos_low\"] = np.zeros(len(df))\n",
    "df[\"pos_med\"] = np.zeros(len(df))\n",
    "df[\"pos_high\"] = np.zeros(len(df))\n",
    "df[\"neg_low\"] = np.zeros(len(df))\n",
    "df[\"neg_med\"] = np.zeros(len(df))\n",
    "df[\"neg_high\"] = np.zeros(len(df))\n",
    "df[\"emo_low\"] = np.zeros(len(df))\n",
    "df[\"emo_med\"] = np.zeros(len(df))\n",
    "df[\"emo_high\"] = np.zeros(len(df))\n",
    "df[\"rep_punc_low\"] = np.zeros(len(df))\n",
    "df[\"rep_punc_med\"] = np.zeros(len(df))\n",
    "df[\"rep_punc_high\"] = np.zeros(len(df))\n",
    "df[\"rep_seq_low\"] = np.zeros(len(df))\n",
    "df[\"rep_seq_med\"] = np.zeros(len(df))\n",
    "df[\"rep_seq_high\"] = np.zeros(len(df))\n",
    "df[\"cap_low\"] = np.zeros(len(df))\n",
    "df[\"cap_med\"] = np.zeros(len(df))\n",
    "df[\"cap_high\"] = np.zeros(len(df))\n",
    "df[\"slang_low\"] = np.zeros(len(df))\n",
    "df[\"slang_med\"] = np.zeros(len(df))\n",
    "df[\"slang_high\"] = np.zeros(len(df))\n",
    "df[\"exclaim_low\"] = np.zeros(len(df))\n",
    "df[\"exclaim_med\"] = np.zeros(len(df))\n",
    "df[\"exclaim_high\"] = np.zeros(len(df))\n",
    "df[\"idioms_low\"] = np.zeros(len(df))\n",
    "df[\"idioms_med\"] = np.zeros(len(df))\n",
    "df[\"idioms_high\"] = np.zeros(len(df))\n",
    "boosterAndSlangs = [\"Lit\", \"Fleek\", \"Slay\", \"Woke\", \"Stan\", \"Chill\", \"On fleek\", \"Squad\", \"Bae\", \"AF\", \"Savage\", \"GOAT\", \"Lit AF\", \"Yas\", \"Gucci\", \"Thirsty\", \"Mood\", \"Extra\", \"Clap back\", \"Shook\", \"Lowkey\", \"Highkey\", \"Basic\", \"Lituation\", \"Snatched\", \"Throwing shade\", \"Swag\", \"Tea\", \"Glow up\", \"Fam\", \"Turnt\", \"Litty\", \"Dope\", \"Hundo P\", \"Gassed\", \"FOMO\", \"Trill\", \"No cap\", \"Blessed\", \"Fire\", \"Wavy\", \"Sus\", \"Tight\", \"Meme\", \"Shade\", \"Receipts\", \"Slay queen\", \"Cray\", \"Thick\", \"Litmas\", \"Litmus\", \"Queen\", \"Bad\", \"No chill\", \"Sorry not sorry\", \"Real talk\", \"Dank\", \"Ship\", \"Ratchet\", \"Yolo\", \"Fierce\", \"Legendary\", \"Drama\", \"Stuntin\", \"Lit fam\", \"Flame\", \"Finna\", \"Swole\", \"Squad goals\", \"Kween\", \"Salty\", \"Slaying\", \"Bounce\", \"Swerve\", \"Bussin\", \"Hype\", \"Finesse\", \"Bless up\", \"Crushin it\", \"Yaas\", \"Fleeky\", \"Fuego\", \"Cringy\", \"Dead\", \"Curve\", \"Baller\", \"Wig snatched\", \"Keep it 100\", \"Hater\", \"My bad\"]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Contradiction Feature: <br>\n",
    "<emsp>We use two binary features Contra and Contra_Coher<br>\n",
    "<emsp>Contra if headline has one sentence and contradiction in sentiment score occur\n",
    "<br>\n",
    "<emsp>Contra_Coher if headline has more than one sentence, contradiction of polarity and the headline is judged coherent<br>\n",
    "\n",
    "### Sentiment Feature <br>\n",
    "<emsp>Calculates the +ve and -ve score of the headline and then classify it as low/med/high\n",
    "\n",
    "### Punctuations and Symbol Features <br>\n",
    "<emsp>We use 7 indicators<br><br>\n",
    "    <emsp><emsp>1. Number of emoticons <br>\n",
    "    <emsp><emsp>2. Number of repetitive sequence of punctuations<br>\n",
    "    <emsp><emsp>3. Number of repetitive sequence of characters<br>\n",
    "    <emsp><emsp>4. Number of capitalized word<br>\n",
    "    <emsp><emsp>5. Number of slang and booster words<br>\n",
    "    <emsp><emsp>6. Number of exclamation marks<br>\n",
    "    <emsp><emsp>7. Number of idioms<br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_symbols(line):\n",
    "    return ''.join(ch for ch in line if ch.isalnum() or ch == \" \")\n",
    "def calculate_scores(sentence):\n",
    "    print(\"Sentence: \",sentence)\n",
    "    score=[]\n",
    "    results = []\n",
    "    for word in nltk.word_tokenize(sentence):\n",
    "        results.append(wScore(word))\n",
    "    positiveSum = positiveScore(results)\n",
    "    negativeSum = negativeScore(results)\n",
    "    score.append(positiveSum)\n",
    "    score.append(negativeSum)\n",
    "    print(\"positiveScore: \",positiveSum)\n",
    "    print(\"negativeScore: \",negativeSum)\n",
    "    return score\n",
    "\n",
    "def isContradiction(scores):\n",
    "    if scores[0]!=0 and scores[1]!=0:\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "def checkCoherence(sentence):\n",
    "    tokens = nltk.sent_tokenize(sentence)\n",
    "    if len(tokens) > 1:\n",
    "        if hasAntecedents(sentence):\n",
    "            return True\n",
    "        w1 = extractSubject(tokens[0])\n",
    "        w2 = extractSubject(tokens[1])\n",
    "        if identicalPronouns(w1,w2) or identicalSubjects(w1,w2) or definiteNounPhraseFeature(tokens[1],w2) or demonstrativeNounPhraseFeature(tokens[1],w2) or properNameFeature(w1,w2):\n",
    "            return True   \n",
    "    return False\n",
    "\n",
    "def countEmoticons(headline):\n",
    "    return len(re.findall(r'[^\\w\\s,]', headline))\n",
    "\n",
    "def countRepititivePunctuations(headline):\n",
    "    return len(re.findall(r'([\\W_]){2,}', headline))\n",
    "\n",
    "def countRepititiveSequences(headline):\n",
    "    return len(re.findall(r'(\\S)\\1{1,}', headline))\n",
    "\n",
    "def countCapitalLetters(headline):\n",
    "    return len(re.findall(r'[A-Z]', headline))\n",
    "\n",
    "def countBoostersAndSlangs(headline):\n",
    "    numSlangsBoosters = 0\n",
    "    for word in headline.split():\n",
    "        if word.lower() in boosterAndSlangs:\n",
    "            numSlangsBoosters += 1\n",
    "    return numSlangsBoosters\n",
    "\n",
    "def countIdioms(headline):\n",
    "    numIdioms = 0\n",
    "    for word in headline.split():\n",
    "        if word.lower() in idioms:\n",
    "            numIdioms += 1\n",
    "    return numIdioms\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def assignSentimentFeature(headline,scores):\n",
    "    positiveScore = scores[0]\n",
    "    negativeScore = scores[1]\n",
    "    if positiveScore <= -1:\n",
    "        df.loc[df[\"headline\"] == headline, \"pos_low\"] = 1\n",
    "    elif positiveScore >= 0 and positiveScore <= 1:\n",
    "        df.loc[df[\"headline\"] == headline, \"pos_med\"] = 1\n",
    "    elif positiveScore >= 2:\n",
    "        df.loc[df[\"headline\"] == headline, \"pos_high\"] = 1\n",
    "    if negativeScore >= 1:\n",
    "        df.loc[df[\"headline\"] == headline, \"neg_low\"] = 1\n",
    "    elif negativeScore >= 0 and negativeScore <= 1:\n",
    "        df.loc[df[\"headline\"] == headline, \"neg_med\"] = 1\n",
    "    elif negativeScore <= -2:\n",
    "        df.loc[df[\"headline\"] == headline, \"neg_high\"] = 1\n",
    "def punctuationAndSpecialSymbolFeature(headline):\n",
    "    numberOfEmoticons = countEmoticons(headline)\n",
    "    if numberOfEmoticons == 0:\n",
    "        df.loc[df[\"headline\"] == headline, \"emo_low\"] = 1\n",
    "    elif numberOfEmoticons >= 1 and numberOfEmoticons <= 3:\n",
    "        df.loc[df[\"headline\"] == headline, \"emo_med\"] = 1\n",
    "    elif numberOfEmoticons >= 4:\n",
    "        df.loc[df[\"headline\"] == headline, \"emo_high\"] = 1\n",
    "    numberOfPunctuations = countRepititivePunctuations(headline)\n",
    "    if numberOfPunctuations == 0:\n",
    "        df.loc[df[\"headline\"] == headline, \"rep_punc_low\"] = 1\n",
    "    elif numberOfPunctuations >= 1 and numberOfPunctuations <= 3:\n",
    "        df.loc[df[\"headline\"] == headline, \"rep_punc_med\"] = 1\n",
    "    elif numberOfPunctuations >= 4:\n",
    "        df.loc[df[\"headline\"] == headline, \"rep_punc_high\"] = 1\n",
    "    numberOfRepetitiveSequences = countRepititiveSequences(headline)\n",
    "    if numberOfRepetitiveSequences == 0:\n",
    "        df.loc[df[\"headline\"] == headline, \"rep_seq_low\"] = 1\n",
    "    elif numberOfRepetitiveSequences >= 1 and numberOfRepetitiveSequences <= 3:\n",
    "        df.loc[df[\"headline\"] == headline, \"rep_seq_med\"] = 1\n",
    "    elif numberOfRepetitiveSequences >= 4:\n",
    "        df.loc[df[\"headline\"] == headline, \"rep_seq_high\"] = 1\n",
    "    numberOfCapitalLetters = countCapitalLetters(headline)\n",
    "    if numberOfCapitalLetters == 0:\n",
    "        df.loc[df[\"headline\"] == headline, \"cap_low\"] = 1\n",
    "    elif numberOfCapitalLetters >= 1 and numberOfCapitalLetters <= 3:\n",
    "        df.loc[df[\"headline\"] == headline, \"cap_med\"] = 1\n",
    "    elif numberOfCapitalLetters >= 4:\n",
    "        df.loc[df[\"headline\"] == headline, \"cap_high\"] = 1\n",
    "    numberOfBoostersAndSlangs = countBoostersAndSlangs(headline)\n",
    "    if numberOfBoostersAndSlangs == 0:\n",
    "        df.loc[df[\"headline\"] == headline, \"slang_low\"] = 1\n",
    "    elif numberOfBoostersAndSlangs >= 1 and numberOfBoostersAndSlangs <= 3:\n",
    "        df.loc[df[\"headline\"] == headline, \"slang_med\"] = 1\n",
    "    elif numberOfBoostersAndSlangs >= 4:\n",
    "        df.loc[df[\"headline\"] == headline, \"slang_high\"] = 1\n",
    "    numberOfIdioms = countIdioms(headline)\n",
    "    if numberOfIdioms == 0:\n",
    "        df.loc[df[\"headline\"] == headline, \"idiom_low\"] = 1\n",
    "    elif numberOfIdioms >= 1 and numberOfIdioms <= 3:\n",
    "        df.loc[df[\"headline\"] == headline, \"idiom_med\"] = 1\n",
    "    elif numberOfIdioms >= 4:\n",
    "        df.loc[df[\"headline\"] == headline, \"idiom_high\"] = 1\n",
    "    \n",
    "def contradictionFeature():\n",
    "    for headline in df[\"headline\"]:\n",
    "        text = remove_symbols(headline)\n",
    "        sentences = nltk.sent_tokenize(text)\n",
    "        scores = calculate_scores(text)\n",
    "        assignSentimentFeature(headline,scores)\n",
    "        punctuationAndSpecialSymbolFeature(headline)\n",
    "        if len(sentences) > 1:\n",
    "            print(\"CONTRA_PLUS_COHER\")\n",
    "            if isContradiction(scores) and checkCoherence(text):\n",
    "                df.loc[df[\"headline\"] == headline, \"CONTRA_PLUS_COHER\"] = 1\n",
    "            else:\n",
    "                df.loc[df[\"headline\"] == headline, \"CONTRA_PLUS_COHER\"] = 0\n",
    "        else:\n",
    "            print(\"CONTRA\")\n",
    "            if isContradiction(scores):\n",
    "                df.loc[df[\"headline\"] == headline, \"CONTRA\"] = 1\n",
    "            else:\n",
    "                df.loc[df[\"headline\"] == headline, \"CONTRA\"] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "listDFs = []\n",
    "for i in range(0,26000,1000):\n",
    "    temp = df[i:i+1000]\n",
    "    listDFs.append(temp)\n",
    "listDFs.append(df[26000:])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rangee= 20\n",
    "for i in range(2):\n",
    "    df = listDFs[rangee + i]\n",
    "    contradictionFeature()\n",
    "    print(df.head())\n",
    "    df.to_csv(\"tempContra/df\"+str(rangee+1+i)+\".csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "length=12\n",
    "counter = 0\n",
    "for i in range(length):\n",
    "    df = pd.read_csv(\"tempContra/df\"+str(i+1)+\".csv\")\n",
    "    sarcastic = df[\"is_sarcastic\"]\n",
    "    results1 = df[\"CONTRA\"]\n",
    "    results2 = df[\"CONTRA_PLUS_COHER\"]\n",
    "    for i in range(len(sarcastic)):\n",
    "        if sarcastic[i] == 1:\n",
    "            if sarcastic[i] == results1[i] or sarcastic[i] == results2[i]:\n",
    "                counter += 1\n",
    "        else:\n",
    "            if sarcastic[i] == results1[i] and sarcastic[i] == results2[i]:\n",
    "                counter += 1\n",
    "                sarcastic = df[\"is_sarcastic\"]\n",
    "print((counter/(length*1000)) * 100)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
