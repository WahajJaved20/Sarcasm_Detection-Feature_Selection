{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install pandas\n",
    "%pip install stanfordnlp\n",
    "%pip install senticnet\n",
    "%pip install sentistrength\n",
    "%pip install nltk\n",
    "%pip install spacy\n",
    "# run this in the terminal\n",
    "# python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "# from stanfordcorenlp import StanfordCoreNLP\n",
    "import requests\n",
    "from senticnet.senticnet import SenticNet\n",
    "from sentistrength import PySentiStr\n",
    "import nltk\n",
    "import spacy\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import re\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('punkt')\n",
    "# stanfordNLP = StanfordCoreNLP(\"http://localhost\", port=8000, timeout=30000)\n",
    "spacyNLP = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download the zip file from the link https://drive.google.com/file/d/1yvCpB2URy0iFjQPn3RmidNOryTlo6vHG/view?usp=share_link\n",
    "# extract the zip file and place the folder in the same directory as this file then cd into the folder\n",
    "# run the following command in the terminal to start the server\n",
    "# java -mx4g -cp \"*\" edu.stanford.stanfordNLP.pipeline.StanfordCoreNLPServer -port {8000 or any port} -timeout 30000\n",
    "# can speed it up by replace 4g with 8g (it represents the ram being used in gigs)\n",
    "def lemmatize(text):\n",
    "    # perform lemmatization\n",
    "    lemmas = []\n",
    "    output = stanfordNLP.annotate(text, properties={'annotators': 'tokenize,lemma', 'outputFormat': 'json'})\n",
    "    output_dict = json.loads(output)\n",
    "    tokens = output_dict['sentences'][0]['tokens']\n",
    "    for token in tokens:\n",
    "        lemmas.append(token['lemma'])\n",
    "   \n",
    "    return lemmas  "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Converting the given JSON file into actual JSON format for easier readbility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "writeFile = open(\"Sarcasm_Headlines.json\", \"w\")\n",
    "writeFile.write(\"{ \\\"headlines\\\": [\")\n",
    "with open(\"Sarcasm_Headlines_Dataset.json\") as readFile:\n",
    "  for item in readFile:\n",
    "    writeFile.write(item + \",\")\n",
    "# removed the final comma manually\n",
    "writeFile.write(\"]}\")\n",
    "readFile.close()\n",
    "writeFile.close()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing Stage"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reading the dataset and removing all article links as our goal is to analyze the headlines for sarcasm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = json.load(open(\"Sarcasm_Headlines.json\"))\n",
    "df = pd.DataFrame(dataset[\"headlines\"])\n",
    "df.drop([\"article_link\"], axis = 1, inplace = True)\n",
    "df.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### lemmatizing the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatizeDataset():\n",
    "    for index, row in df.iterrows():\n",
    "        sentence = row['headline']\n",
    "        row['headline'] = lemmatize(sentence)\n",
    "\n",
    "lemmatizeDataset()\n",
    "df.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### writing to a csv file to avoid having to perform pre-processing again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('lemmatized.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"lemmatized.csv\")\n",
    "df.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 1 => Concept Level and Common Sense Knowledge\n",
    "### ConceptNet\n",
    "ConceptNet is a semantic network consisting of common-sense knowledge and concepts, represented<br> in the form of nodes (words or\n",
    "short phrases) and labeled edges (relationships) between them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# set the API endpoint and parameters\n",
    "endpoint = 'http://api.conceptnet.io/c/en/'\n",
    "params = {\n",
    "    'filter': 'core',\n",
    "    'limit': 1000\n",
    "}\n",
    "def conceptNet(sentence):\n",
    "    # send a GET request to the API endpoint\n",
    "    response = requests.get(endpoint + sentence, params=params)\n",
    "\n",
    "    # parse the JSON response\n",
    "    data = json.loads(response.text)\n",
    "    edges = data['edges']\n",
    "    edges.sort(key=lambda x: x['weight'], reverse=True)\n",
    "\n",
    "    return edges"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 2 => Sentiment Score\n",
    "### SentiStrength\n",
    "SentiStrength is a sentiment lexicon that uses linguistic information and rules to detect<br>\n",
    "sentiment strength in English text. SentiStrength provides positive and negative sentiment<br>\n",
    "scores for each word. Both scores are integers from 1 to 5, where 1 signifies weak sentiment<br>\n",
    "and 5 signifies strong sentiment.\n",
    "<br>\n",
    "polarity = positiveSentiment - negativeSentiment\n",
    "\n",
    "### SenticNet\n",
    "SenticNet is a resource for opinion mining that aims to create a collection of commonly<br> \n",
    "used common-sense concepts  with positive and negative sentiment scores. The sentiment <br>\n",
    "score for each word is scaled from -1 to 1, where -1 signifies strongly negative sentiment,<br>\n",
    "0 signifies neutral sentiment and 1 signifies strong positive sentiment.\n",
    "<br> sentiment = score * 5 (in-order to keep it with sentiStrength)\n",
    "\n",
    "### Rules of w_score (sentiment score) selection:\n",
    "- if word belongs to SentiStrength || SenticNet => pick the score whichever exists\n",
    "- if word belongs to SentiStrength && SenticNet => avg score of the lexicons\n",
    "- else get the concepts from concept net to expand the meaning => select top 5 ranked and calculate the avg sentiment score\n",
    "\n",
    "### Final Calculation\n",
    "sum_pos_score = sum of all positive sentiment scores<br>\n",
    "sum_neg_score = sum of all negative sentiment scores<br>\n",
    "if sum_pos_score && sum_neg_score > 0, there is a contradiction in the sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sn = SenticNet()\n",
    "def senticNetScore(word):\n",
    "    try:\n",
    "        polarityValue = sn.polarity_value(word)\n",
    "        return float(polarityValue) * 5\n",
    "    except KeyError:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "senti = PySentiStr()\n",
    "# got the jar file and data folder from the author (also reverse engineered the pysenti package to extract the jar file)\n",
    "senti.setSentiStrengthPath('C:/Users/pd/OneDrive/Desktop/IR project/Sarcasm_Detection-Feature_Selection/SentiStrengthCom.jar')\n",
    "senti.setSentiStrengthLanguageFolderPath('C:/Users/pd/OneDrive/Desktop/IR project/Sarcasm_Detection-Feature_Selection/SentStrength_Data')\n",
    "def sentiStrengthScore(word):\n",
    "    result = senti.getSentiment(word)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wScore(word):\n",
    "    senticNet = senticNetScore(word)\n",
    "    sentiStrength = sentiStrengthScore(word)[0]\n",
    "    if senticNet == None and sentiStrength == None:\n",
    "        expansion = conceptNet(word)\n",
    "        if len(expansion) == 0:\n",
    "            return 0\n",
    "        else:\n",
    "            score = 0\n",
    "            expansion = expansion[:5]\n",
    "            for edge in expansion:\n",
    "                score += wScore(edge['end']['label'])\n",
    "            return score / 5\n",
    "    elif senticNet == None:\n",
    "        return sentiStrength\n",
    "    elif sentiStrength == None:\n",
    "        return senticNet\n",
    "    else:\n",
    "        return (senticNet + sentiStrength) / 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def positiveScore(results):\n",
    "    score = 0\n",
    "    for result in results:\n",
    "        if result > 0:\n",
    "            score += result\n",
    "    return score\n",
    "def negativeScore(results):\n",
    "    score = 0\n",
    "    for result in results:\n",
    "        if result < 0:\n",
    "            score += result\n",
    "    return score"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 3 => Sentence Coherence\n",
    "Checking the coreference between subjects or objects of a sentence\n",
    "<br> for two subjects w1 and w2, sentence is coherent if\n",
    "- if w1 is antecedent of w2\n",
    "- if w1 and w2 are identical pronouns\n",
    "- if w1 and w2 are identical subjects\n",
    "- w2 starts with the word \"the\" (Definite Noun Phrase)\n",
    "- w2 starts with \"this\", \"that\", \"these\", \"those\" (Demonstrative Noun Phrases)\n",
    "- if w1 and w2 are proper nouns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extractSubject(sentence):\n",
    "    doc = spacyNLP(sentence)\n",
    "    subject = None\n",
    "    for token in doc:\n",
    "        if token.dep_ == \"nsubj\":\n",
    "            subject = token.text\n",
    "    return subject"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hasAntecedents(text):\n",
    "    doc = spacyNLP(text)\n",
    "    antecedents = []\n",
    "    for token in doc:\n",
    "        if token.dep_ == \"nsubj\" and token.head.pos_ == \"VERB\":\n",
    "            for mention in doc.ents:\n",
    "                if mention.start <= token.i < mention.end:\n",
    "                    antecedents.append(mention.text)\n",
    "    return antecedents\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pronounLemmatizer = WordNetLemmatizer()\n",
    "def identicalPronouns(w1, w2):\n",
    "    lemma1 = pronounLemmatizer.lemmatize(w1, 'n')\n",
    "    lemma2 = pronounLemmatizer.lemmatize(w2, 'n')\n",
    "    if lemma1 == lemma2:\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def identicalSubjects(w1,w2):\n",
    "    cleanedSubject1 = re.sub(r'[^a-zA-Z]', '', w1)\n",
    "    cleanedSubject2 = re.sub(r'[^a-zA-Z]', '', w2)\n",
    "    if cleanedSubject1 == cleanedSubject2:\n",
    "        return True\n",
    "    else:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def definiteNounPhraseFeature(text,w2):\n",
    "    doc = nltk.word_tokenize(text)\n",
    "    for i in range(len(doc)):\n",
    "        if i-1 >= 0 and doc[i] == w2:\n",
    "            if doc[i-1] == 'the':\n",
    "                return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def demonstrativeNounPhraseFeature(text,w2):\n",
    "    doc = nltk.word_tokenize(text)\n",
    "    for i in range(len(doc)):\n",
    "        if doc[i] == w2:\n",
    "            if i-1 >= 0 and doc[i-1] == 'this' or doc[i-1] == 'that' or doc[i-1] == 'these' or doc[i-1] == 'those':\n",
    "                return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def properNameFeature(w1,w2):\n",
    "    taggedWords = nltk.pos_tag([w1,w2])\n",
    "    proper = False\n",
    "    for word, tag in taggedWords:\n",
    "        if tag in ['NNP', 'NNPS']:\n",
    "            proper = True\n",
    "        else:\n",
    "            proper = False\n",
    "            break\n",
    "    return proper"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 4 Creation of Feature Vector\n",
    "\n",
    "We need to do following feature classificiation on each headline\n",
    "- Contradiction Feature: <br>\n",
    "<emsp>We use two binary features Contra and Contra_Coher<br>\n",
    "<emsp>Contra if headline has one sentence and contradiction in sentiment score occur\n",
    "<br>\n",
    "<emsp>Contra_Coher if headline has more than one sentence, contradiction of polarity and the tweet is judged coherent<br>\n",
    "- Sentiment Feature <br>\n",
    "<emsp>Calculates the +ve and -ve score of the headline and then classify it as low/med/high\n",
    "- Punctuation <br>\n",
    "<emsp>We use 7 indicators<br><br>\n",
    "    <emsp><emsp>1. Number of emoticons <br>\n",
    "    <emsp><emsp>2. Number of repetitive sequence of punctuations<br>\n",
    "    <emsp><emsp>3. Number of repetitive sequence of characters<br>\n",
    "    <emsp><emsp>4. Number of capitalized word<br>\n",
    "    <emsp><emsp>5. Number of slang and booster words<br>\n",
    "    <emsp><emsp>6. Number of exclamation marks<br>\n",
    "    <emsp><emsp>7. Number of idioms<br"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('lemmatized.csv')\n",
    "df = df.drop('is_sarcastic',axis='columns')\n",
    "sentences = df['headline']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_symbols(line):\n",
    "    return ''.join(ch for ch in line if ch.isalnum())\n",
    "\n",
    "def calculate_scores(sentence):\n",
    "    score=[]\n",
    "    results = []\n",
    "    for i in range(len(sentence)):\n",
    "        results[i] = wScore(sentence[i])\n",
    "    score[0]=positiveScore(results)\n",
    "    score[1]=negativeScore(results)\n",
    "    return score\n",
    "\n",
    "def isContradiction(scores):\n",
    "    if scores[0]!=0 and scores[1]!=0:\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "def checkCoherence(sentence):\n",
    "    if hasAntecedents(\".\".join(sentence)) is not None:\n",
    "        return True\n",
    "    for i in range(len(sentence))-1:\n",
    "        s1 = sentence[i]\n",
    "        s2 = sentence[i+1]\n",
    "        w1 = extractSubject(s1)\n",
    "        w2 = extractSubject(s2)\n",
    "        \n",
    "        if identicalPronouns(w1,w2) or identicalSubjects(w1,w2) or definiteNounPhraseFeature(s2,w2) or demonstrativeNounPhraseFeature(s2,w2) or properNameFeature(w1,w2):\n",
    "            return True\n",
    "    \n",
    "    return False\n",
    "        \n",
    "\n",
    "for sentence in sentences:\n",
    "    # if only one sentence\n",
    "    scores = []\n",
    "    if '.' not in sentence:\n",
    "        sentence = sentence.split(' ')\n",
    "        for i in range(len(sentence)):\n",
    "            sentence[i] = remove_symbols(sentence[i])\n",
    "        scores = calculate_scores(sentence)\n",
    "        if isContradiction(scores):\n",
    "            df['contra'] = 1\n",
    "        else:\n",
    "            df['contra'] = 0\n",
    "        df['contra_coher'] = 0\n",
    "    # if more than one sentence\n",
    "    else:\n",
    "        sentence = sentence.split('.')\n",
    "        if checkCoherence(sentence):\n",
    "            for i in range(len(sentence)):\n",
    "                temp_sentence = sentence[i]\n",
    "                temp_sentence = temp_sentence.split(' ')\n",
    "                for j in range(len(temp_sentence)):\n",
    "                    temp_sentence[i] = remove_symbols(temp_sentence[i])\n",
    "                scores = calculate_scores(temp_sentence)\n",
    "                if isContradiction(scores):\n",
    "                    df['contra_coher'] = 1\n",
    "                else:\n",
    "                    df['contra_coher'] = 0\n",
    "        df['contra'] = 0\n",
    "    ## implementing 4.4.3\n",
    "\n",
    "    ## positive score\n",
    "    if score[0] < -1:\n",
    "        df['pos_low'] = 1\n",
    "        df['pos_medium'] = 0\n",
    "        df['pos_high'] = 0\n",
    "    elif score[0] >= 0 and score[0]<=1:\n",
    "        df['pos_low'] = 0\n",
    "        df['pos_medium'] = 1\n",
    "        df['pos_high'] = 0\n",
    "    elif score[0] >= 2:\n",
    "        df['pos_low'] = 0\n",
    "        df['pos_medium'] = 0\n",
    "        df['pos_high'] = 1\n",
    "    else:\n",
    "        df['pos_low'] = 0\n",
    "        df['pos_medium'] = 0\n",
    "        df['pos_high'] = 0\n",
    "    \n",
    "    ## negative score\n",
    "    if score[1] < -1:\n",
    "        df['neg_low'] = 1\n",
    "        df['neg_medium'] = 0\n",
    "        df['neg_high'] = 0\n",
    "    elif score[1] >= 0 and score[1]<=1:\n",
    "        df['neg_low'] = 0\n",
    "        df['neg_medium'] = 1\n",
    "        df['neg_high'] = 0\n",
    "    elif score[1] >= 2:\n",
    "        df['neg_low'] = 0\n",
    "        df['neg_medium'] = 0\n",
    "        df['neg_high'] = 1\n",
    "    else:\n",
    "        df['neg_low'] = 0\n",
    "        df['neg_medium'] = 0\n",
    "        df['neg_high'] = 0\n",
    "df\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
